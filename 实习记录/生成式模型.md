# Attention is All You Need
## 前言
```
We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.
```
RNN 和 长短记忆都不好用的时候，Transformer 出手了。
一开始是作为翻译而存在的，脱胎自RNN上的注意力机制，Transformer诞生了。Transformer完全依赖于注意力机制
## 背景
多头注意力，自注意力，端到端记忆。而transformer是完全依赖于多头注意力机制的，这点与RNN和卷积不同。
## 数学模型
其它的竞品模型，都有编码器-解码器层。而Transformer没有。

