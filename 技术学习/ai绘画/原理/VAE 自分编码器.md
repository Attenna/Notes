[什么是变分自编码器 (VAE) ？| IBM](https://www.ibm.com/cn-zh/think/topics/variational-autoencoder)
## 什么是变分自编码器 (VAE) ？

变分自编码器 (VAE) 这种[生成式模型](https://www.ibm.com/cn-zh/topics/generative-ai)用于[机器学习](https://www.ibm.com/cn-zh/topics/machine-learning) (ML) 中，能以训练所输入数据的_变体_形式生成新数据。除此之外， VAE 还执行其他自编码器常见的任务，例如去噪。

与所有[自编码器](https://www.ibm.com/cn-zh/topics/autoencoder)一样，变分自编码器 (VAE) 是[深度学习](https://www.ibm.com/cn-zh/topics/deep-learning)模型，由一个编码器和一个解码器组成；前者学习将重要的潜在变量与训练数据隔离开来，然后后者使用这些潜在变量来重建输入数据。  
  
然而，大多数自编码器架构对潜在变量的_离散_、固定表示进行编码，而 VAE 对该潜在空间的_连续_、概率表示进行编码。这使得变分自编码器 (VAE) 不仅能够准确重建确切的原始输入，还可以使用变分推理生成类似于原始输入数据的新数据样本。

变分自编码器 (VAE) 的[神经网络](https://www.ibm.com/cn-zh/topics/neural-networks)架构最初是 Diederik P. Kingma 和 Max Welling 于 2013 年发表的一篇题为《[自编码变分贝叶斯](https://arxiv.org/abs/1312.6114)》的论文中提出的（ibm.com 外部链接）。本文还宣传了他们所谓的_重新参数化技巧_，这是一种重要的机器学习技术，可以在不影响模型可微性（即优化模型参数的能力）的情况下使用随机性作为模型输入。

虽然变分自编码器 (VAE) 经常在图像生成语境中进行讨论，包括在本文中，但它们可用于各种人工智能 (AI) 应用，从[异常检测](https://www.ibm.com/cn-zh/topics/anomaly-detection)1 到生成新药物[分子](https://www.nature.com/articles/s42004-023-01054-6)2（ibm.com 外部链接）。

![使用手写数字演示变分自编码器 (VAE) 的输出结果](https://www.ibm.com/content/dam/connectedassets-adobe-cms/worldwide-content/creative-assets/s-migr/ul/g/d4/b1/variational-autoencoder-manifold-traversal.png)

顶部：来自 MNIST 数据集的手写数字；底部：在这些 MNIST 数字上训练的 VAE 输出的原始样本。来源：Mak、Hugo 与 Han、Runze 与 Yin, Hoover。（2023 年）

## 什么是潜在空间？

对于理解变分自编码器 (VAE) 或任何其他类型的自编码器至关重要的是_潜在空间_的概念，即对一组特定输入数据的集体_潜在变量_的称呼。简而言之，潜在变量是数据的基础变量，它们告知数据的分布方式，但通常无法直接观察。

为了对潜在变量的概念进行有用的可视化，想象一座桥上有一个传感器，可以测量每辆过往车辆的重量。当然，使用这座桥的车辆种类繁多，从轻型小敞篷车到重型大卡车。由于没有摄像头，我们无法检测特定车辆是敞篷车、轿车、货车还是卡车。但是，我们确实知道车辆的类型会对车辆的重量产生很大影响。

因此，这个例子需要两个随机变量，**x** 和 **z**，其中 **x** 是车辆重量这一直接可观测变量，**z** 是车辆类型这一潜在变量。任何自编码器的主要训练目标都是让它学会如何对特定输入的潜在空间进行有效建模。

### 潜在空间和降维

自动编码器通过[_降维_](https://www.ibm.com/cn-zh/topics/dimensionality-reduction)对潜在空间进行建模：将数据压缩到较低维空间，以捕获原始输入中包含的有意义信息。

在机器学习 (ML) 语境中，数学维度不对应于我们熟悉的物理世界空间维度，而是对应于数据特征。例如，[MNIST 数据集](https://dataplatform.cloud.ibm.com/exchange/public/entry/view/e5ec6df59cd736a6b6b091062e3715a2?context=cpdaas)中一张 28x28 像素的手写数字黑白图像可以表示为一个 784 维矢量，其中每个维度对应一个单独的像素，其值范围从 0（表示黑色）到 1（表示白色）。同样的“彩色”图像可以表示为一个 2,352 维的矢量，其中 784 个像素中的每个像素都有三个维度，分别对应其红、绿、蓝 (RGB) 值。

但是，并非所有这些维度都包含有用的信息。实际数字本身仅代表图像的一小部分，因此大部分_输入空间_是背景噪声。将数据压缩到仅包含相关信息的维度（_潜在空间_）可以提高许多 ML 任务和算法的准确性、效率和功效。

![自动编码器的视觉描述](https://www.ibm.com/content/dam/connectedassets-adobe-cms/worldwide-content/creative-assets/s-migr/ul/g/b6/f4/variational-autoencoder-neural-network.png)

标准自编码器神经网络架构的视觉描述。

## 什么是自动编码器？

[自编码器](https://www.ibm.com/cn-zh/topics/autoencoder)是[深度学习](https://www.ibm.com/cn-zh/topics/neural-networks)中的一种[神经网络](https://www.ibm.com/cn-zh/topics/deep-learning)架构，通常用于数据压缩、图像去噪、异常检测和面部识别等任务。

自动编码器是[自监督](https://www.ibm.com/cn-zh/topics/self-supervised-learning)系统，其训练目标是通过降维来压缩（或“编码”）输入数据，然后使用该压缩表示准确重建（或“解码”）其原始输入。

从根本上讲，自动编码器的功能是有效地提取数据中最显著的信息——其潜在变量——并丢弃不相关的噪声。不同类型的自动编码器之间的区别在于它们用于提取信息的特定策略以及各自策略最适合的用例。

在训练中，编码器网络将来自训练数据集的输入数据通过“瓶颈”传递到解码器。反过来，解码器网络负责仅使用潜在变量矢量来重建原始输入。

每个训练周期后，都会使用[梯度下降](https://www.ibm.com/cn-zh/topics/gradient-descent)等优化算法来调整模型权重，以最小化原始数据输入和解码器输出之间的差异。最终，编码器学会了允许最有利于准确重建的信息通过，而解码器学会了有效地重建它。

虽然这最直观地适用于直接的数据压缩任务，但对未标记数据的准确潜在表示进行有效编码的能力使自动编码器具有广泛的应用。例如，自动编码器可用于恢复损坏的音频文件、为灰度图像着色或检测肉眼不可见的异常（例如由欺诈引起的异常）。

### 自动编码器结构

尽管不同类型的自编码器会添加或改变其架构的某些方面以更好地适应特定目标和数据类型，但所有自编码器都有三个关键的结构元素：

**编码器**提取输入数据 _x_ 的潜在变量，并以表示潜在空间 _z_ 的矢量形式输出。在典型的“基本”自编码器中，编码器的每个后续层包含的节点逐渐少于前一层；当数据遍历每个编码器层时，它会通过将自身“挤压”为更小维度的过程进行压缩。

其他自编码器变体则使用正则项，如通过惩罚每层激活的节点数量来强制_稀疏性_的函数，来实现降维。

**瓶颈**或**“代码”**既是编码器网络的输出层，也是解码器网络的输入层。它包含潜在空间：输入数据的完全压缩的低维嵌入。足够的瓶颈是必要的，以帮助确保解码器不能简单地复制或记忆输入数据；这名义上可以满足其训练任务，但却会阻止自编码器学习。

**解码器**使用该潜在表示，通过实质上反转编码器来重建原始输入：在典型的解码器架构中，每个后续层都包含逐渐_增加_的活动节点数量。

虽然许多自编码器的编码器和解码器网络是由标准多层感知器 (MLP) 构建的，但自编码器不限于任何特定类型的 Neural Networks。

用于[计算机视觉](https://www.ibm.com/cn-zh/topics/computer-vision)任务的自动编码器通常是[卷积神经网络 (CNN)](https://www.ibm.com/cn-zh/topics/convolutional-neural-networks)，因此被称为卷积自动编码器。基于[转换器架构](https://www.ibm.com/cn-zh/topics/transformer-model)构建的自动编码器已用于多个领域，包括计算机视觉3 和音乐。4  

与其他降维算法（例如[主成分分析 (PCA)](https://www.ibm.com/cn-zh/topics/principal-component-analysis)）相比，自动编码器的一个主要优点是，自动编码器可以对不同变量之间的“非线性”关系进行建模。因此，自动编码器神经网络的节点通常使用非线性激活函数。

在许多自编码器应用中，解码器仅用于辅助编码器的优化，因此在训练后被丢弃。在变分自编码器 (VAE) 中，解码器被保留并用于生成新的数据点。

## 变分自编码器 (VAE) 如何工作？

变分自编码器 (VAE) 与其他自编码器的区别在于 VAE 对潜在空间进行编码的独特方式，以及可以应用其概率编码的不同用例。

与大多数自动编码器不同，VAES _是概率__模型，_而 VAES 是概率模型。VAE 不会将训练数据的潜在变量编码为固定的离散值**z**，而是以概率分布**_p_(_z_)**表示的连续可能性范围。

在[贝叶斯统计](https://www.ibm.com/cn-zh/topics/naive-bayes#A+brief+review+of+Bayesian+statistics)中，潜在变量的这种学习到的可能性范围称为_先验分布_。在_变分推理_，即合成新数据点的生成过程中，该先验分布用于计算_后验分布_ **_p_(_z_|_x_)。**换句话说，给定潜在变量 **z** 的值时，可观察变量 **x** 的值。

对于训练数据的每个潜在属性，VAE 编码两个不同的潜在矢量：均值矢量“_**μ**_”和标准差矢量“_**σ**_”。本质上，这两个矢量表示每个潜在变量的可能性范围以及每个可能性范围内的预期方差。

通过从该编码可能性范围内随机采样，VAE 可以合成新的数据样本，这些数据样本虽然本身独一无二且原创，但与原始训练数据相似。尽管从原理上来说相对直观，但该方法需要进一步适应标准自编码器方法才能付诸实践。

为了解释 VAE 的这种能力，我们将审视以下概念：  

- **重建损失  
      
    **
- **Kullback-Leibler (KL) 散度  
      
    **
- **证据下界 (ELBO)  
      
    **
- **重新参数化技巧**  
    

### 重建损失

与所有自动编码器一样，VAE 使用_重建损失_（也称为_重建误差_）作为训练中的主要损失函数。重建误差测量原始输入数据与解码器输出的该数据的重建版本之间的差异（或“损失”）。多种算法（包括交叉熵损失或均方误差 (MSE)）可以用作重建损失函数。

如前所述，自编码器架构产生了一个瓶颈，只允许原始输入数据的子集传递到解码器。在训练开始时，通常从模型参数的随机初始化开始，编码器尚未了解数据的哪些部分应该更重要。因此，它最初会输出次优的潜在表示，而解码器会输出原始输入的相当不准确或不完整的重建。

通过对编码器网络和解码器网络的参数进行某种形式的梯度下降来尽量减少重建误差，自动编码器模型的权重将以产生更有用的潜在空间编码的方式进行调整（从而获得更准确的重建）。从数学上讲，重建损失函数的目标是优化_p_ θ ( _z_ | _x_ )，其中_**θ**_表示强制在给定潜在变量**z**的情况下准确重建输入**x**的模型参数。

仅重建损失就足以优化大多数自编码器，其唯一目标是学习有助于准确重建的输入数据的压缩后表示。

然而，变分自编码器 (VAE) 的目标不是重建原始输入，而是为了生成_类似于_原始输入的_新_样本。出于这个原因，需要一个额外的优化项。

### Kullback-Leibler 散度

对于变分推理（通过训练模型生成新样本）而言，仅重建损失就可能会导致潜在空间的不规则编码，从而[过拟合](https://www.ibm.com/cn-zh/topics/overfitting)训练数据，不能很好地推广到新样本。因此，VAE 包含另一个正则项：_Kullback-Leibler 散度_，简称 KL 散度。

为了生成图像，解码器会从潜空间采样。从表示训练数据中原始输入的潜在空间中特定点采样将复制这些原始输入。要生成_新_图像，VAE 必须能够从原始数据点_之间_的潜在空间中的任何位置进行采样。要做到这一点，潜在空间必须表现出两种正则性：

- **连续性：** 在解码时，潜在空间中的邻近点应产生相似的内容。
- **完整性：**从潜在空间采样的任何点在解码时都应产生有意义的内容。

在潜空间中实现连续性和完整性的一个简单方法是帮助确保它遵循标准正态分布，即_高斯分布_。但是，仅最小化重建损失并不能激励模型以任何特定方式组织潜在空间，因为“中间”空间与原始数据点的准确重建无关。这就是 KL 散度正则项发挥作用的地方。

KL 散度是用于比较两种概率分布的指标。最小化潜在变量的习得分布与值范围为 0 到 1 的简单高斯分布之间的 KL 散度，会强制潜在变量的习得编码服从正态分布。这允许对潜在空间中的任何点进行平滑内插，从而生成新图像。

![仅使用重建损失、仅使用 KL 散度以及两者组合的潜在空间比较](https://www.ibm.com/content/dam/connectedassets-adobe-cms/worldwide-content/creative-assets/s-migr/ul/g/d8/41/variational-autoencoder-distribution.png)

示例说明重建损失和 KL 差异如何影响 MNIST 数据集中手写数字 0-9 的潜在空间建模。

### 证据下界 (ELBO)

使用 KL 散度进行变分推理的一个障碍是方程的分母_难以_计算，这意味着直接计算所需的时间理论上是无限的。为了解决这个问题，并整合两个关键损失函数，VAE 通过_最大化__证据下界_ (ELBO) 来近似_最小化_ KL 散度。

在统计术语中，“证据下界”中的“证据”是指 _p_(_x_)，即 VAE 表面上负责重建的可观察输入数据。输入数据中的那些可观察变量是自编码器发现的潜在变量的“证据”。“下界”是指给定分布的对数似然的_最坏情况估计值_。实际对数似然值可能高于 ELBO。

在 VAE 的语境中，证据下界是指特定后验分布（换句话说，自编码器的特定输出，受到 KL 散度损失项和重建损失项的影响）符合训练数据的“证据”的可能性的最坏情况估计。因此，训练变分推理模型可以称为最大化 ELBO。

### 重新参数化技巧

如前所述，变分推理的目标是以训练数据 **x** 的随机变化形式输出新数据。初看起来，这相对简单：使用函数 _ƒ_ 为潜在变量 _z_ 选择一个随机值，然后解码器可以使用该值生成 _x_ 的近似重构值。

然而，随机性的一个固有特性是它无法进行优化。没有“最好”的随机数，根据定义，随机值组成的矢量没有导数，即在结果模型输出中没有表示任何模式的梯度，因此无法使用任何形式的梯度下降通过反向传播进行优化。这意味着使用前面的随机采样过程的神经网络无法学习最佳参数来完成其任务。

为了避开这个障碍，VAE 使用了重新参数化技巧。重新参数化技巧引入了一个新参数 **_ε_**，它是从 0 到 1 之间的正态分布中选择的随机值。

然后，它将潜在变量 _z_ 重新参数化为 _z_ = _μx_ + _εσx_。简而言之，它通过从潜在变量 z 的平均值（用 _μ_ 表示）开始移动标准差 (_σ_) 的随机倍数（用 _ε_ 表示）来为潜在变量 _z_ 选择一个值。以 _z_ 的特定值为条件，解码器输出新的样本。

由于随机值_ε_并非来自于自动编码器模型参数，也与自动编码器模型参数无关，因此在反向传播过程中可以忽略它。该模型通过某种形式的梯度下降进行更新 - 最常见的是通过[Adam](https://arxiv.org/abs/1412.6980)（链接位于 ibm.com 之外），这是一种基于梯度的优化算法，也是由 Kingma 开发的 - 以尽可能提高 ELBO。

## 条件性 VAE (CVAE)

传统“原始” VAE 的一个缺点是用户无法控制自动编码器生成的特定输出。例如，使用前面提到的 MNIST 数据集训练的传统 VAE，可以生成从 0 到 9 的手写数字新样本，但不能局限于仅输出 4 和 7。

顾名思义，条件性 VAE (CVAE) 可以以特定输入为_条件_进行输出，而不仅仅是随机生成训练数据的变体。这是通过将[监督学习](https://www.ibm.com/cn-zh/topics/supervised-learning)（或[半监督学习](https://www.ibm.com/cn-zh/topics/semi-supervised-learning)）的元素与常规自编码器的传统[无监督](https://www.ibm.com/cn-zh/topics/unsupervised-learning)训练目标相结合来实现的。

通过在特定变量的标记示例上进一步训练模型，这些变量可用于调节解码器的输出。例如，CVAE 可首先在面部图像的大数据集上进行训练，然后使用监督学习进行训练，学习“胡须”的潜在编码，从而输出新的有胡须面部的图像。

行业时事通讯

### 专家为您带来最新的 AI 趋势

获取有关最重要且最有趣的 AI 新闻的精选洞察分析。订阅我们的每周 Think 时事通讯。请参阅 [IBM 隐私声明](https://www.ibm.com/cn-zh/privacy)。

- 商业电子邮件
    

订阅

## VAE 与 GAN

VAE 经常与生成式对抗网络 (GAN) 进行比较，GAN 是另一种模型架构，用于生成类似于训练数据的样本，尤其是图像。

与 VAE 类似，GAN 是结合两种神经网络的联合架构：一个_生成器_网络，负责输出与训练数据集中的图像相似的图像样本，另一个_判别器_网络，负责确定特定图像是训练数据中的“真实”图像还是来自生成器网络的“虚假”图像。

这两个网络在零和博弈中进行对抗性训练：来自判别器的反馈用于改进生成器的输出，直到判别器不再能够区分真假样本。

就图像合成而言，两者各有优劣：GAN 可以生成更清晰的图像，但由于两种复合模型之间的对抗性权衡，在训练中并不稳定。VAE 更容易训练，但由于其根据训练数据的“平均”特征生成图像的性质，往往会生成比较模糊的图像。

### VAE-GAN

顾名思义，VAE-GAN 是变分自动编码器 (VAE) 和生成式对抗网络 (GAN) 的混合体。通过用判别器网络替换 VAE 模型的重建损失项，来降低 VAE 生成图像的模糊性。